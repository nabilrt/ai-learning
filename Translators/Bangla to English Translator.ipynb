{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "913c35b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DATA_TO_LOAD = 3000\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "EMBEDDING_DIM = 256\n",
    "UNITS = 1024\n",
    "\n",
    "MAX_INPUT_LANG_LEN = 13\n",
    "MAX_TARGET_LANG_LEN = 10\n",
    "\n",
    "RESTORE_SAVED_CHECKPOINT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dbc8adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import unicodedata\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing import text\n",
    "\n",
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def clean_seq(w):\n",
    "  w = unicode_to_ascii(w.lower().strip())\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "  w = re.sub(r\"([?.!,।])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  w = re.sub(r\"[^a-zA-Z ঁ -৯।?.!,]+\", \" \", w)\n",
    "\n",
    "  w = w.strip()\n",
    "\n",
    "  return w\n",
    "\n",
    "def add_start_and_end_token_to_seq(sentence):\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    return '<start> ' + sentence + ' <end>'\n",
    "\n",
    "def texts_to_sequences(texts, tokenizer):\n",
    "    tensor = tokenizer.texts_to_sequences(texts)\n",
    "    tensor = sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "def get_lang_tokenize(texts):\n",
    "    lang_tokenizer = text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    return lang_tokenizer\n",
    "\n",
    "def save_tokenizer(tokenizer, save_at, file_name):\n",
    "    path_to_file = os.path.join(save_at, file_name)\n",
    "    with open(path_to_file, 'w', encoding='utf8') as fp:\n",
    "        tokenizer_json = tokenizer.to_json()\n",
    "        fp.write(json.dumps(tokenizer_json, indent=4, ensure_ascii=False))\n",
    "    print(\"Tokenizer write at:\", path_to_file)\n",
    "\n",
    "def load_tokenizer(path_to_tokenizer_file):\n",
    "    print(\"Loading:\", path_to_tokenizer_file)\n",
    "    with open(path_to_tokenizer_file, 'r', encoding='utf8') as fp:\n",
    "        tokenizer_json = json.load(fp)\n",
    "        tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(tokenizer_json)\n",
    "    return tokenizer\n",
    " \n",
    "def show_index_to_word_maping(inp_lang_tok, tensor):\n",
    "    print('-' * 45)\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            print(\"{:<6} map with {}\".format(\n",
    "                t, inp_lang_tok.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61efb0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "class TatoebaDataset():\n",
    "    def __init__(self, path_to_file, num_data_to_load):\n",
    "        self.path_to_file = path_to_file\n",
    "        self.num_data_to_load = num_data_to_load\n",
    "\n",
    "    def read_data(self):\n",
    "        lines = io.open(\n",
    "            self.path_to_file, encoding='UTF-8').read().strip().split('\\n')\n",
    "        return lines\n",
    "\n",
    "    def make_sequence_pair(self, lines):\n",
    "        seq_pairs = []\n",
    "        for line in lines[:self.num_data_to_load]:\n",
    "            en, bn, _ = line.split('\\t')\n",
    "            pair = []\n",
    "            for seq in [en, bn]:\n",
    "                seq = clean_seq(seq)\n",
    "                seq = add_start_and_end_token_to_seq(seq)\n",
    "                pair.append(seq)    \n",
    "            seq_pairs.append(pair)\n",
    "        return seq_pairs\n",
    "\n",
    "    def create_dataset(self):\n",
    "        lines = self.read_data()\n",
    "        word_pairs = self.make_sequence_pair(lines)\n",
    "        return zip(*word_pairs)\n",
    "    \n",
    "    def load_data(self):\n",
    "        # creating cleaned input, output pairs\n",
    "        targ_lang_text, inp_lang_text = self.create_dataset()\n",
    "\n",
    "        targ_lang_tokenizer = get_lang_tokenize(targ_lang_text)\n",
    "        inp_lang_tokenizer = get_lang_tokenize(inp_lang_text)\n",
    "        \n",
    "        target_tensor = texts_to_sequences(targ_lang_text, targ_lang_tokenizer)\n",
    "        input_tensor  = texts_to_sequences(inp_lang_text, inp_lang_tokenizer)\n",
    "       \n",
    "        tensor_pair = (input_tensor, target_tensor)\n",
    "        tokenizer_pair = (inp_lang_tokenizer, targ_lang_tokenizer)\n",
    "\n",
    "        return tensor_pair, tokenizer_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86358376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                        return_sequences=True,\n",
    "                                        return_state=True,\n",
    "                                        recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, dec_hidden, enc_output):\n",
    "        # dec_hidden hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # enc_output shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(dec_hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(enc_output)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                        return_sequences=True,\n",
    "                                        return_state=True,\n",
    "                                        recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, dec_input, dec_hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(dec_hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(dec_input)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56ca8787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "from matplotlib import ticker\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import font_manager as fm\n",
    "\n",
    "FONT_NAME = 'assets/banglafonts/Siyamrupali.ttf'\n",
    "\n",
    "class Infer():\n",
    "    def __init__(self, input_language_tokenizer, target_language_tokenizer,\n",
    "                max_length_input, max_length_target, encoder, decoder, units):\n",
    "        self.input_language_tokenizer = input_language_tokenizer\n",
    "        self.target_language_tokenizer = target_language_tokenizer\n",
    "        self.max_length_input = max_length_input\n",
    "        self.max_length_target = max_length_target\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.units = units\n",
    "    \n",
    "    def preprocess(self, sentence):\n",
    "        # clean and pad sequece\n",
    "        sentence = clean_seq(sentence)\n",
    "        sentence = add_start_and_end_token_to_seq(sentence)\n",
    "        \n",
    "        inputs = [\n",
    "            self.input_language_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "        inputs = sequence.pad_sequences(\n",
    "            [inputs], maxlen=self.max_length_input,padding='post')\n",
    "        tensor = tf.convert_to_tensor(inputs)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        tensor = self.preprocess(sentence)\n",
    "\n",
    "        # init encoder\n",
    "        encoder_initial_hidden = [tf.zeros((1, self.units))]\n",
    "        encoder_out, encoder_hidden = self.encoder(tensor, encoder_initial_hidden)\n",
    "\n",
    "        # init decoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_input = tf.expand_dims(\n",
    "            [self.target_language_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "        result = ''\n",
    "        for _ in range(self.max_length_target):\n",
    "            predictions, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_out)\n",
    "            predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "            result += self.target_language_tokenizer.index_word[predicted_id] + ' '\n",
    "            if self.target_language_tokenizer.index_word[predicted_id] == '<end>':\n",
    "                return result\n",
    "            # the predicted ID is fed back into the model insteqad of using \n",
    "            # teacher forcing that we use in training time\n",
    "            decoder_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def predict_with_attention_weights(self, sentence):\n",
    "        tensor = self.preprocess(sentence)\n",
    "\n",
    "        # init encoder\n",
    "        encoder_initial_hidden = [tf.zeros((1, self.units))]\n",
    "        encoder_out, encoder_hidden = self.encoder(tensor, encoder_initial_hidden)\n",
    "\n",
    "        # init decoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_input = tf.expand_dims(\n",
    "            [self.target_language_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "        result = ''\n",
    "        attention_plot = np.zeros((self.max_length_target, self.max_length_input))\n",
    "        for t in range(self.max_length_target):\n",
    "            predictions, decoder_hidden, attention_weights = \\\n",
    "                self.decoder(decoder_input, decoder_hidden, encoder_out)\n",
    "            \n",
    "            # storing the attention weights to plot later on\n",
    "            attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "            attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "            predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "            result += self.target_language_tokenizer.index_word[predicted_id] + ' '\n",
    "            if self.target_language_tokenizer.index_word[predicted_id] == '<end>':\n",
    "                return result, sentence, attention_plot\n",
    "\n",
    "            # the predicted ID is fed back into the model insteqad of using \n",
    "            # teacher forcing that we use in training time\n",
    "            decoder_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        return result, sentence, attention_plot\n",
    "\n",
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    prop = fm.FontProperties(fname=FONT_NAME)\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, rotation=90, fontproperties=prop)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontproperties=prop)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "492aef70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer write at: ./input_language_tokenizer.json\n",
      "Tokenizer write at: ./target_language_tokenizer.json\n",
      "Epoch 0 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 2.0797 - : 100%|########################################################| 37/37 [00:50<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 2.5266\n",
      "Epoch 1 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 1.8703 - : 100%|########################################################| 37/37 [00:40<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 1.9780\n",
      "Epoch 2 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 1.7172 - : 100%|########################################################| 37/37 [00:43<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 1.7801\n",
      "Epoch 3 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 1.5614 - : 100%|########################################################| 37/37 [00:52<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 1.6270\n",
      "Epoch 4 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 1.4837 - : 100%|########################################################| 37/37 [00:42<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 1.4258\n",
      "Epoch 5 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 1.2631 - : 100%|########################################################| 37/37 [00:42<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 1.2766\n",
      "Epoch 6 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 1.1683 - : 100%|########################################################| 37/37 [00:40<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 1.1595\n",
      "Epoch 7 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 1.0984 - : 100%|########################################################| 37/37 [00:41<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 1.0546\n",
      "Epoch 8 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.9845 - : 100%|########################################################| 37/37 [00:40<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.9684\n",
      "Epoch 9 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.9307 - : 100%|########################################################| 37/37 [00:50<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.8888\n",
      "Epoch 10 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.8168 - : 100%|########################################################| 37/37 [00:41<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.8201\n",
      "Epoch 11 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.7672 - : 100%|########################################################| 37/37 [00:41<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.7449\n",
      "Epoch 12 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.7635 - : 100%|########################################################| 37/37 [00:41<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.6693\n",
      "Epoch 13 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.6130 - : 100%|########################################################| 37/37 [00:41<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.5969\n",
      "Epoch 14 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.5438 - : 100%|########################################################| 37/37 [00:40<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.5236\n",
      "Epoch 15 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.4785 - : 100%|########################################################| 37/37 [00:40<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.4557\n",
      "Epoch 16 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.3857 - : 100%|########################################################| 37/37 [00:42<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.3939\n",
      "Epoch 17 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.4548 - : 100%|########################################################| 37/37 [00:41<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.3392\n",
      "Epoch 18 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.3054 - : 100%|########################################################| 37/37 [00:42<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.2958\n",
      "Epoch 19 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.2229 - : 100%|########################################################| 37/37 [00:40<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.2559\n",
      "Epoch 20 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.2403 - : 100%|########################################################| 37/37 [00:47<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.2219\n",
      "Epoch 21 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.2229 - : 100%|########################################################| 37/37 [00:46<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.1918\n",
      "Epoch 22 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.1706 - : 100%|########################################################| 37/37 [00:42<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.1678\n",
      "Epoch 23 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.1446 - : 100%|########################################################| 37/37 [00:41<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.1426\n",
      "Epoch 24 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.1357 - : 100%|########################################################| 37/37 [00:41<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.1209\n",
      "Epoch 25 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.1002 - : 100%|########################################################| 37/37 [00:41<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.1007\n",
      "Epoch 26 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0808 - : 100%|########################################################| 37/37 [00:40<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0813\n",
      "Epoch 27 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0680 - : 100%|########################################################| 37/37 [00:41<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0704\n",
      "Epoch 28 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0639 - : 100%|########################################################| 37/37 [00:41<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0641\n",
      "Epoch 29 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0722 - : 100%|########################################################| 37/37 [00:40<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0592\n",
      "Epoch 30 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0654 - : 100%|########################################################| 37/37 [00:42<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0567\n",
      "Epoch 31 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0597 - : 100%|########################################################| 37/37 [00:42<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0516\n",
      "Epoch 32 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0483 - : 100%|########################################################| 37/37 [00:42<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0461\n",
      "Epoch 33 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0544 - : 100%|########################################################| 37/37 [00:41<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0485\n",
      "Epoch 34 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0533 - : 100%|########################################################| 37/37 [00:42<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0460\n",
      "Epoch 35 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0717 - : 100%|########################################################| 37/37 [00:41<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0439\n",
      "Epoch 36 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0249 - : 100%|########################################################| 37/37 [00:41<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0391\n",
      "Epoch 37 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0524 - : 100%|########################################################| 37/37 [00:41<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0406\n",
      "Epoch 38 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0361 - : 100%|########################################################| 37/37 [00:48<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0389\n",
      "Epoch 39 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0558 - : 100%|########################################################| 37/37 [00:45<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0371\n",
      "Epoch 40 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0638 - : 100%|########################################################| 37/37 [00:40<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0391\n",
      "Epoch 41 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0684 - : 100%|########################################################| 37/37 [00:41<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0373\n",
      "Epoch 42 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0513 - : 100%|########################################################| 37/37 [00:41<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0361\n",
      "Epoch 43 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0426 - : 100%|########################################################| 37/37 [00:43<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0387\n",
      "Epoch 44 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0200 - : 100%|########################################################| 37/37 [00:42<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0388\n",
      "Epoch 45 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0475 - : 100%|########################################################| 37/37 [00:42<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0393\n",
      "Epoch 46 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0247 - : 100%|########################################################| 37/37 [00:42<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0372\n",
      "Epoch 47 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0351 - : 100%|########################################################| 37/37 [00:41<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0358\n",
      "Epoch 48 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0647 - : 100%|########################################################| 37/37 [00:41<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0410\n",
      "Epoch 49 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0528 - : 100%|########################################################| 37/37 [00:41<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0419\n",
      "Epoch 50 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0661 - : 100%|########################################################| 37/37 [00:41<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0485\n",
      "Epoch 51 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0610 - : 100%|########################################################| 37/37 [00:42<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0545\n",
      "Epoch 52 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0579 - : 100%|########################################################| 37/37 [00:42<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0575\n",
      "Epoch 53 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0595 - : 100%|########################################################| 37/37 [00:41<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0576\n",
      "Epoch 54 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0755 - : 100%|########################################################| 37/37 [00:40<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0600\n",
      "Epoch 55 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0470 - : 100%|########################################################| 37/37 [00:41<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0536\n",
      "Epoch 56 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0546 - : 100%|########################################################| 37/37 [00:42<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0452\n",
      "Epoch 57 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0360 - : 100%|########################################################| 37/37 [00:40<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0410\n",
      "Epoch 58 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0413 - : 100%|########################################################| 37/37 [00:41<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0371\n",
      "Epoch 59 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0479 - : 100%|########################################################| 37/37 [00:41<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0337\n",
      "Epoch 60 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0332 - : 100%|########################################################| 37/37 [00:42<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0324\n",
      "Epoch 61 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0343 - : 100%|########################################################| 37/37 [00:46<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0299\n",
      "Epoch 62 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0297 - : 100%|########################################################| 37/37 [00:42<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0293\n",
      "Epoch 63 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0179 - : 100%|########################################################| 37/37 [00:43<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0294\n",
      "Epoch 64 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0367 - : 100%|########################################################| 37/37 [00:41<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0293\n",
      "Epoch 65 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0454 - : 100%|########################################################| 37/37 [00:40<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0274\n",
      "Epoch 66 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0296 - : 100%|########################################################| 37/37 [00:42<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0269\n",
      "Epoch 67 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0327 - : 100%|########################################################| 37/37 [00:42<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0269\n",
      "Epoch 68 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0179 - : 100%|########################################################| 37/37 [00:41<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0272\n",
      "Epoch 69 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0280 - : 100%|########################################################| 37/37 [00:42<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0275\n",
      "Epoch 70 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0124 - : 100%|########################################################| 37/37 [00:41<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0270\n",
      "Epoch 71 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0593 - : 100%|########################################################| 37/37 [00:41<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0287\n",
      "Epoch 72 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0468 - : 100%|########################################################| 37/37 [00:41<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0262\n",
      "Epoch 73 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0365 - : 100%|########################################################| 37/37 [00:41<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0254\n",
      "Epoch 74 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0386 - : 100%|########################################################| 37/37 [00:41<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0258\n",
      "Epoch 75 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0465 - : 100%|########################################################| 37/37 [00:42<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0266\n",
      "Epoch 76 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0254 - : 100%|########################################################| 37/37 [00:41<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0266\n",
      "Epoch 77 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0545 - : 100%|########################################################| 37/37 [00:41<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0270\n",
      "Epoch 78 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0260 - : 100%|########################################################| 37/37 [00:41<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0281\n",
      "Epoch 79 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0289 - : 100%|########################################################| 37/37 [00:41<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0269\n",
      "Epoch 80 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0327 - : 100%|########################################################| 37/37 [00:41<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0267\n",
      "Epoch 81 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0319 - : 100%|########################################################| 37/37 [00:41<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0315\n",
      "Epoch 82 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0596 - : 100%|########################################################| 37/37 [00:48<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0490\n",
      "Epoch 83 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.1088 - : 100%|########################################################| 37/37 [00:46<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0715\n",
      "Epoch 84 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.1101 - : 100%|########################################################| 37/37 [00:41<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0900\n",
      "Epoch 85 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.1044 - : 100%|########################################################| 37/37 [00:40<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0986\n",
      "Epoch 86 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.1074 - : 100%|########################################################| 37/37 [00:41<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0779\n",
      "Epoch 87 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0655 - : 100%|########################################################| 37/37 [00:42<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0607\n",
      "Epoch 88 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0400 - : 100%|########################################################| 37/37 [00:44<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0508\n",
      "Epoch 89 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0322 - : 100%|########################################################| 37/37 [00:43<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0396\n",
      "Epoch 90 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0570 - : 100%|########################################################| 37/37 [00:42<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0341\n",
      "Epoch 91 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0255 - : 100%|########################################################| 37/37 [00:41<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0327\n",
      "Epoch 92 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0505 - : 100%|########################################################| 37/37 [00:42<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0300\n",
      "Epoch 93 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0095 - : 100%|########################################################| 37/37 [00:41<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0284\n",
      "Epoch 94 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0151 - : 100%|########################################################| 37/37 [00:42<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0281\n",
      "Epoch 95 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0358 - : 100%|########################################################| 37/37 [00:42<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0257\n",
      "Epoch 96 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0392 - : 100%|########################################################| 37/37 [00:41<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0269\n",
      "Epoch 97 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0426 - : 100%|########################################################| 37/37 [00:43<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0240\n",
      "Epoch 98 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0224 - : 100%|########################################################| 37/37 [00:41<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0258\n",
      "Epoch 99 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step - 37 / 37 - batch loss - 0.0286 - : 100%|########################################################| 37/37 [00:41<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss - 0.0249\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, targ, targ_lang_tokenizer, \n",
    "            enc_hidden, encoder, decoder, optimizer):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims(\n",
    "            [targ_lang_tokenizer.word_index['<start>']]*BATCH_SIZE, 1)\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss\n",
    "\n",
    "def run():\n",
    "    text_data = TatoebaDataset('./ben-eng/ben.txt', NUM_DATA_TO_LOAD)\n",
    "    \n",
    "    # retrive data and tokenizers\n",
    "    tensors, tokenizer = text_data.load_data()\n",
    "    input_tensor, target_tensor = tensors \n",
    "    inp_lang_tokenizer, targ_lang_tokenizer = tokenizer\n",
    "\n",
    "    # save tokenizer for further use\n",
    "    save_tokenizer(\n",
    "        tokenizer=inp_lang_tokenizer,\n",
    "        save_at='./',\n",
    "        file_name='input_language_tokenizer.json')\n",
    "    save_tokenizer(\n",
    "        tokenizer=targ_lang_tokenizer,\n",
    "        save_at='./',\n",
    "        file_name='target_language_tokenizer.json')  \n",
    "\n",
    "    # Creating training and validation sets using an 80-20 split\n",
    "    input_train, input_val, target_train, target_val = \\\n",
    "        train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "    # set training params\n",
    "    buffer_size = len(input_train)\n",
    "    steps_per_epoch = len(input_train) // BATCH_SIZE\n",
    "    vocab_inp_size = len(inp_lang_tokenizer.word_index) + 1\n",
    "    vocab_tar_size = len(targ_lang_tokenizer.word_index) + 1\n",
    "\n",
    "    # convert data to tf.data formate\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_train, target_train))\n",
    "    dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "    # init optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    \n",
    "    # init encoder & decoder\n",
    "    encoder = Encoder(\n",
    "        vocab_inp_size, EMBEDDING_DIM, UNITS, BATCH_SIZE)\n",
    "    decoder = Decoder(\n",
    "        vocab_tar_size, EMBEDDING_DIM, UNITS, BATCH_SIZE)\n",
    "\n",
    "    # init checkpoint \n",
    "    checkpoint_dir = './training_checkpoints'\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                    encoder=encoder,\n",
    "                                    decoder=decoder)\n",
    "\n",
    "    if RESTORE_SAVED_CHECKPOINT:\n",
    "        checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(\"Epoch {} / {}\".format(epoch, EPOCHS))\n",
    "        pbar = tqdm(dataset.take(steps_per_epoch), ascii=True)\n",
    "        \n",
    "        total_loss = 0\n",
    "        enc_hidden = encoder.initialize_hidden_state()\n",
    "        \n",
    "        for step, data in enumerate(pbar):\n",
    "            inp, targ = data\n",
    "            batch_loss = train_step(\n",
    "                inp, targ, targ_lang_tokenizer,\n",
    "                enc_hidden, encoder, decoder, optimizer)\n",
    "            \n",
    "            total_loss += batch_loss\n",
    "\n",
    "            pbar.set_description(\n",
    "                \"Step - {} / {} - batch loss - {:.4f} - \"\n",
    "                    .format(steps_per_epoch, step+1, batch_loss.numpy()))\n",
    "        \n",
    "        # saving (checkpoint) the model every 2 epochs\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "        print('Epoch loss - {:.4f}'.format(total_loss / steps_per_epoch))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6917c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./input_language_tokenizer.json\n",
      "Loading: ./target_language_tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "# load input and terget language tokenizer\n",
    "input_language_tokenizer = load_tokenizer('./input_language_tokenizer.json')\n",
    "target_language_tokenizer = load_tokenizer('./target_language_tokenizer.json')\n",
    "\n",
    "# init vocab size for input and terget language\n",
    "vocab_inp_size = len(input_language_tokenizer.word_index)+1\n",
    "vocab_tar_size = len(target_language_tokenizer.word_index)+1\n",
    "\n",
    "# init encoder & decoder model\n",
    "encoder = Encoder(vocab_inp_size, EMBEDDING_DIM, UNITS, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, EMBEDDING_DIM, UNITS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "664765df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x1960167a050>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint = tf.train.Checkpoint(encoder=encoder, decoder=decoder)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "116b33a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Infer(\n",
    "    input_language_tokenizer=input_language_tokenizer,\n",
    "    target_language_tokenizer=target_language_tokenizer,\n",
    "    max_length_input=MAX_INPUT_LANG_LEN,\n",
    "    max_length_target=MAX_TARGET_LANG_LEN,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    units=UNITS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e8d9e303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text: help us . <end> \n"
     ]
    }
   ],
   "source": [
    "translated_text = predictor.predict(\"আমাদের সাহায্য করুন।\")\n",
    "print(\"Translated text: {}\".format(translated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c06058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
